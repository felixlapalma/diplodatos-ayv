{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio: Analizar texto\n",
    "\n",
    "Utilizar un texto de proyecto Gutenberg en castellano (http://www.gutenberg.org/browse/languages/es)\n",
    "\n",
    "Contar palabras y ordenar por frecuencia\n",
    "- Limpiar preludio y licencia de Project Gutenberg\n",
    "- Omitir “palabras vacías” (stop words) y símbolos\n",
    "\n",
    "Encontrar personajes\n",
    "\n",
    "Hacer un análisis extra a gusto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# función auxiliar\n",
    "#https://relopezbriega.github.io/blog/2017/09/23/procesamiento-del-lenguaje-natural-con-python/\n",
    "def leer_texto(texto):\n",
    "    \"\"\"Funcion auxiliar para leer un archivo de texto\"\"\"\n",
    "    with open(texto, 'r') as text:\n",
    "        return text.read()\n",
    "    \n",
    "def inStr_idx(textin,strLst):\n",
    "    s_id_ini=[]\n",
    "    s_id_end=[]\n",
    "    for s in strLst:\n",
    "        s_len=len(s)\n",
    "        if str.find(textin,s)!=-1:\n",
    "            s_id_ini.append(str.find(textin,s))\n",
    "            s_id_end.append(str.find(textin,s)+s_len)\n",
    "        else:\n",
    "            s_id_ini.append(-1)\n",
    "            s_id_end.append(-1)\n",
    "    return s_id_ini,s_id_end\n",
    "\n",
    "#https://relopezbriega.github.io/blog/2017/09/23/procesamiento-del-lenguaje-natural-con-python/\n",
    "def encontrar_personajes(doc):\n",
    "    \"\"\"\n",
    "    Devuelve una lista de los personajes de un `doc` con su cantidad de\n",
    "    ocurrencias\n",
    "    \n",
    "    :param doc: NLP documento parseado por Spacy\n",
    "    :return: Lista de Tuplas con la forma\n",
    "        [('winston', 686), (\"o'brien\", 135), ('julia', 85),]\n",
    "    \"\"\"\n",
    "    personajes = Counter()\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PER':\n",
    "            personajes[ent.lemma_] += 1\n",
    "            \n",
    "    return personajes.most_common()\n",
    "\n",
    "#https://relopezbriega.github.io/blog/2017/09/23/procesamiento-del-lenguaje-natural-con-python/\n",
    "def obtener_adj_pers(doc, personaje):\n",
    "    \"\"\"\n",
    "    Encontrar todos los adjetivos relacionados a un personaje en un `doc`\n",
    "    \n",
    "    :param doc: NLP documento parseado por Spacy\n",
    "    :param personaje: un objeto String \n",
    "    :return: lista de adjetivos relacionados a un `personaje`\n",
    "    \"\"\"\n",
    "    \n",
    "    adjetivos = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.lemma_ == personaje:\n",
    "            for token in ent.subtree:\n",
    "                if token.pos_ == 'ADJ':\n",
    "                    adjetivos.append(token.lemma_)\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.lemma_ == personaje:\n",
    "            if ent.root.dep_ == 'nsubj':\n",
    "                for child in ent.root.head.children:\n",
    "                    if child.dep_ == 'acomp':\n",
    "                        adjetivos.append(child.lemma_)\n",
    "    \n",
    "    return adjetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Limpiar preludio y licencia de Project Gutenberg*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[762, 397086]\n",
      "[827, 397110]\n",
      "[-1, -1]\n",
      "[-1, -1]\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el texto\n",
    "texto_raw = leer_texto('pg43033.txt')\n",
    "# Vemos de limpiar la parte de proyecto Gutenberg. Si abrimos el documento observamos un par de frases en las cuales parecen marcar el limite\n",
    "# ['Nota del transcriptor: La ortografía del original fue conservada.','End of Project Gutenberg']\n",
    "# Entonces buscamos los indices final de la primera e inicial de la segunda y nos quedamos con la seccion del medio\n",
    "ini_idx,end_idx=inStr_idx(texto_raw,['Nota del transcriptor: La ortografía del original fue conservada.','End of Project Gutenberg'])\n",
    "print(ini_idx)\n",
    "print(end_idx)\n",
    "texto_ini=texto_raw[end_idx[0]:ini_idx[1]]\n",
    "# Y si los buscamos nuevamente no deberiamos encontrarlos (-1 en inStr_idx)\n",
    "ini_idx,end_idx=inStr_idx(texto_ini,['Nota del transcriptor: La ortografía del original fue conservada.','End of Project Gutenberg'])\n",
    "print(ini_idx)\n",
    "print(end_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Cargamos los modulos e instanciamos el modelo en español*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "# Cargando el modelo en español de spacy\n",
    "nlp = textacy.load_spacy('es_core_news_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Limpiamos un poco mas el texto. Seguimos *\n",
    "- https://textacy.readthedocs.io/en/stable/api_reference.html#module-textacy.preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizamos espacios en blanco\n",
    "texto_nwp=textacy.preprocess.normalize_whitespace(texto_ini)\n",
    "#Y Seguimos\n",
    "texto_prc=textacy.preprocess.preprocess_text(texto_nwp, fix_unicode=True, lowercase=False, transliterate=False, no_urls=False, no_emails=False, no_phone_numbers=True, no_numbers=False, no_currency_symbols=True, no_punct=True, no_contractions=False, no_accents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora lo incorporamos en un doc de textacy\n",
    "doc=textacy.Doc(texto_prc,lang=nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_terms=doc.to_bag_of_terms(ngrams=(1), named_entities=True, normalize=u'lemma', lemmatize=None, lowercase=None, weighting=u'freq', as_strings=True,filter_stops=True,filter_punct=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Las listas ordenadas por frecuencia serian:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De : https://www.pythoncentral.io/how-to-sort-python-dictionaries-by-key-or-value/\n",
    "bag_terms_list_sorted_min_max=sorted(bag_terms, key=bag_terms.__getitem__,reverse=False)\n",
    "bag_terms_list_sorted_max_min=sorted(bag_terms, key=bag_terms.__getitem__,reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Buscamos los personajes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LOC', 'MISC', 'PER', 'ORG']\n",
      "838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Penso',\n",
       " 'Deten',\n",
       " 'Cojo Le',\n",
       " 'Garro Ortiz',\n",
       " 'Ortiz Y lo Salvadora',\n",
       " 'Asi',\n",
       " 'Madrid',\n",
       " 'Salga tú Salio',\n",
       " 'Sanchez del Pelgar Era']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Usamos extract.named_entities\n",
    "ents=textacy.extract.named_entities(doc)\n",
    "ents_per=[]\n",
    "ents_lemma_set=[]\n",
    "for x in ents:\n",
    "    ents_lemma_set.append(x.label_)\n",
    "    #En https://textacy.readthedocs.io/en/stable/api_reference.html#module-textacy.extract menciona PERSON, pero al ver los posibles, lemmas solo PER existe como opcion...\n",
    "    if x.label_=='PER':\n",
    "        ents_per.append(x.lemma_)\n",
    "ents_lemma_set=list(set(ents_lemma_set))\n",
    "ents_per=list(set(ents_per))\n",
    "print(ents_lemma_set)\n",
    "print(len(ents_per))\n",
    "ents_per[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Usamos alternativamente la funcion del tutorial*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "844"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ents_per_func=encontrar_personajes(doc.spacy_doc)\n",
    "len(ents_per_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Vemos que tenemos alguna diferencia en el metodo. Veamos de donde sale...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "entlist=[x[0] for x in ents_per_func]\n",
    "diff_list_a=[];diff_list_b=[]\n",
    "for x in entlist:\n",
    "    if x in ents_per:\n",
    "        pass\n",
    "    else:\n",
    "        diff_list_a.append(x)\n",
    "for x in ents_per:\n",
    "    if x in entlist:\n",
    "        pass\n",
    "    else:\n",
    "        diff_list_b.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['El Bizco',\n",
       " 'El',\n",
       " 'La',\n",
       " 'El Garro',\n",
       " 'Aquel Bayardo',\n",
       " 'Las \\n cuco',\n",
       " 'La baronesa de Aynant Paquita Figueroa',\n",
       " 'Los',\n",
       " 'Un']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_list_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bayardo', 'cuco', 'baronesa de Aynant Paquita Figueroa']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_list_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actualmente',\n",
       " 'acuerdo',\n",
       " 'adelante',\n",
       " 'ademas',\n",
       " 'además',\n",
       " 'adrede',\n",
       " 'afirmó',\n",
       " 'agregó',\n",
       " 'ahi',\n",
       " 'ahora',\n",
       " 'ahí',\n",
       " 'al',\n",
       " 'algo',\n",
       " 'alguna',\n",
       " 'algunas',\n",
       " 'alguno',\n",
       " 'algunos',\n",
       " 'algún',\n",
       " 'alli',\n",
       " 'allí',\n",
       " 'alrededor',\n",
       " 'ambos',\n",
       " 'ampleamos',\n",
       " 'antano',\n",
       " 'antaño',\n",
       " 'ante',\n",
       " 'anterior',\n",
       " 'antes',\n",
       " 'apenas',\n",
       " 'aproximadamente',\n",
       " 'aquel',\n",
       " 'aquella',\n",
       " 'aquellas',\n",
       " 'aquello',\n",
       " 'aquellos',\n",
       " 'aqui',\n",
       " 'aquél',\n",
       " 'aquélla',\n",
       " 'aquéllas',\n",
       " 'aquéllos',\n",
       " 'aquí',\n",
       " 'arriba',\n",
       " 'arribaabajo',\n",
       " 'aseguró',\n",
       " 'asi',\n",
       " 'así',\n",
       " 'atras',\n",
       " 'aun',\n",
       " 'aunque',\n",
       " 'ayer',\n",
       " 'añadió',\n",
       " 'aún',\n",
       " 'bajo',\n",
       " 'bastante',\n",
       " 'bien',\n",
       " 'breve',\n",
       " 'buen',\n",
       " 'buena',\n",
       " 'buenas',\n",
       " 'bueno',\n",
       " 'buenos',\n",
       " 'cada',\n",
       " 'casi',\n",
       " 'cerca',\n",
       " 'cierta',\n",
       " 'ciertas',\n",
       " 'cierto',\n",
       " 'ciertos',\n",
       " 'cinco',\n",
       " 'claro',\n",
       " 'comentó',\n",
       " 'como',\n",
       " 'con',\n",
       " 'conmigo',\n",
       " 'conocer',\n",
       " 'conseguimos',\n",
       " 'conseguir',\n",
       " 'considera',\n",
       " 'consideró',\n",
       " 'consigo',\n",
       " 'consigue',\n",
       " 'consiguen',\n",
       " 'consigues',\n",
       " 'contigo',\n",
       " 'contra',\n",
       " 'cosas',\n",
       " 'creo',\n",
       " 'cual',\n",
       " 'cuales',\n",
       " 'cualquier',\n",
       " 'cuando',\n",
       " 'cuanta',\n",
       " 'cuantas',\n",
       " 'cuanto',\n",
       " 'cuantos',\n",
       " 'cuatro',\n",
       " 'cuenta',\n",
       " 'cuál',\n",
       " 'cuáles',\n",
       " 'cuándo',\n",
       " 'cuánta',\n",
       " 'cuántas',\n",
       " 'cuánto',\n",
       " 'cuántos',\n",
       " 'cómo',\n",
       " 'da',\n",
       " 'dado',\n",
       " 'dan',\n",
       " 'dar',\n",
       " 'de',\n",
       " 'debajo',\n",
       " 'debe',\n",
       " 'deben',\n",
       " 'debido',\n",
       " 'decir',\n",
       " 'dejó',\n",
       " 'del',\n",
       " 'delante',\n",
       " 'demasiado',\n",
       " 'demás',\n",
       " 'dentro',\n",
       " 'deprisa',\n",
       " 'desde',\n",
       " 'despacio',\n",
       " 'despues',\n",
       " 'después',\n",
       " 'detras',\n",
       " 'detrás',\n",
       " 'dia',\n",
       " 'dias',\n",
       " 'dice',\n",
       " 'dicen',\n",
       " 'dicho',\n",
       " 'dieron',\n",
       " 'diferente',\n",
       " 'diferentes',\n",
       " 'dijeron',\n",
       " 'dijo',\n",
       " 'dio',\n",
       " 'donde',\n",
       " 'dos',\n",
       " 'durante',\n",
       " 'día',\n",
       " 'días',\n",
       " 'dónde',\n",
       " 'ejemplo',\n",
       " 'el',\n",
       " 'ella',\n",
       " 'ellas',\n",
       " 'ello',\n",
       " 'ellos',\n",
       " 'embargo',\n",
       " 'empleais',\n",
       " 'emplean',\n",
       " 'emplear',\n",
       " 'empleas',\n",
       " 'empleo',\n",
       " 'en',\n",
       " 'encima',\n",
       " 'encuentra',\n",
       " 'enfrente',\n",
       " 'enseguida',\n",
       " 'entonces',\n",
       " 'entre',\n",
       " 'era',\n",
       " 'eramos',\n",
       " 'eran',\n",
       " 'eras',\n",
       " 'eres',\n",
       " 'es',\n",
       " 'esa',\n",
       " 'esas',\n",
       " 'ese',\n",
       " 'eso',\n",
       " 'esos',\n",
       " 'esta',\n",
       " 'estaba',\n",
       " 'estaban',\n",
       " 'estado',\n",
       " 'estados',\n",
       " 'estais',\n",
       " 'estamos',\n",
       " 'estan',\n",
       " 'estar',\n",
       " 'estará',\n",
       " 'estas',\n",
       " 'este',\n",
       " 'esto',\n",
       " 'estos',\n",
       " 'estoy',\n",
       " 'estuvo',\n",
       " 'está',\n",
       " 'están',\n",
       " 'ex',\n",
       " 'excepto',\n",
       " 'existe',\n",
       " 'existen',\n",
       " 'explicó',\n",
       " 'expresó',\n",
       " 'fin',\n",
       " 'final',\n",
       " 'fue',\n",
       " 'fuera',\n",
       " 'fueron',\n",
       " 'fui',\n",
       " 'fuimos',\n",
       " 'general',\n",
       " 'gran',\n",
       " 'grandes',\n",
       " 'gueno',\n",
       " 'ha',\n",
       " 'haber',\n",
       " 'habia',\n",
       " 'habla',\n",
       " 'hablan',\n",
       " 'habrá',\n",
       " 'había',\n",
       " 'habían',\n",
       " 'hace',\n",
       " 'haceis',\n",
       " 'hacemos',\n",
       " 'hacen',\n",
       " 'hacer',\n",
       " 'hacerlo',\n",
       " 'haces',\n",
       " 'hacia',\n",
       " 'haciendo',\n",
       " 'hago',\n",
       " 'han',\n",
       " 'hasta',\n",
       " 'hay',\n",
       " 'haya',\n",
       " 'he',\n",
       " 'hecho',\n",
       " 'hemos',\n",
       " 'hicieron',\n",
       " 'hizo',\n",
       " 'horas',\n",
       " 'hoy',\n",
       " 'hubo',\n",
       " 'igual',\n",
       " 'incluso',\n",
       " 'indicó',\n",
       " 'informo',\n",
       " 'informó',\n",
       " 'intenta',\n",
       " 'intentais',\n",
       " 'intentamos',\n",
       " 'intentan',\n",
       " 'intentar',\n",
       " 'intentas',\n",
       " 'intento',\n",
       " 'ir',\n",
       " 'junto',\n",
       " 'la',\n",
       " 'lado',\n",
       " 'largo',\n",
       " 'las',\n",
       " 'le',\n",
       " 'lejos',\n",
       " 'les',\n",
       " 'llegó',\n",
       " 'lleva',\n",
       " 'llevar',\n",
       " 'lo',\n",
       " 'los',\n",
       " 'luego',\n",
       " 'lugar',\n",
       " 'mal',\n",
       " 'manera',\n",
       " 'manifestó',\n",
       " 'mas',\n",
       " 'mayor',\n",
       " 'me',\n",
       " 'mediante',\n",
       " 'medio',\n",
       " 'mejor',\n",
       " 'mencionó',\n",
       " 'menos',\n",
       " 'menudo',\n",
       " 'mi',\n",
       " 'mia',\n",
       " 'mias',\n",
       " 'mientras',\n",
       " 'mio',\n",
       " 'mios',\n",
       " 'mis',\n",
       " 'misma',\n",
       " 'mismas',\n",
       " 'mismo',\n",
       " 'mismos',\n",
       " 'modo',\n",
       " 'momento',\n",
       " 'mucha',\n",
       " 'muchas',\n",
       " 'mucho',\n",
       " 'muchos',\n",
       " 'muy',\n",
       " 'más',\n",
       " 'mí',\n",
       " 'mía',\n",
       " 'mías',\n",
       " 'mío',\n",
       " 'míos',\n",
       " 'nada',\n",
       " 'nadie',\n",
       " 'ni',\n",
       " 'ninguna',\n",
       " 'ningunas',\n",
       " 'ninguno',\n",
       " 'ningunos',\n",
       " 'ningún',\n",
       " 'no',\n",
       " 'nos',\n",
       " 'nosotras',\n",
       " 'nosotros',\n",
       " 'nuestra',\n",
       " 'nuestras',\n",
       " 'nuestro',\n",
       " 'nuestros',\n",
       " 'nueva',\n",
       " 'nuevas',\n",
       " 'nuevo',\n",
       " 'nuevos',\n",
       " 'nunca',\n",
       " 'ocho',\n",
       " 'os',\n",
       " 'otra',\n",
       " 'otras',\n",
       " 'otro',\n",
       " 'otros',\n",
       " 'pais',\n",
       " 'para',\n",
       " 'parece',\n",
       " 'parte',\n",
       " 'partir',\n",
       " 'pasada',\n",
       " 'pasado',\n",
       " 'paìs',\n",
       " 'peor',\n",
       " 'pero',\n",
       " 'pesar',\n",
       " 'poca',\n",
       " 'pocas',\n",
       " 'poco',\n",
       " 'pocos',\n",
       " 'podeis',\n",
       " 'podemos',\n",
       " 'poder',\n",
       " 'podria',\n",
       " 'podriais',\n",
       " 'podriamos',\n",
       " 'podrian',\n",
       " 'podrias',\n",
       " 'podrá',\n",
       " 'podrán',\n",
       " 'podría',\n",
       " 'podrían',\n",
       " 'poner',\n",
       " 'por',\n",
       " 'porque',\n",
       " 'posible',\n",
       " 'primer',\n",
       " 'primera',\n",
       " 'primero',\n",
       " 'primeros',\n",
       " 'principalmente',\n",
       " 'pronto',\n",
       " 'propia',\n",
       " 'propias',\n",
       " 'propio',\n",
       " 'propios',\n",
       " 'proximo',\n",
       " 'próximo',\n",
       " 'próximos',\n",
       " 'pudo',\n",
       " 'pueda',\n",
       " 'puede',\n",
       " 'pueden',\n",
       " 'puedo',\n",
       " 'pues',\n",
       " 'qeu',\n",
       " 'que',\n",
       " 'quedó',\n",
       " 'queremos',\n",
       " 'quien',\n",
       " 'quienes',\n",
       " 'quiere',\n",
       " 'quiza',\n",
       " 'quizas',\n",
       " 'quizá',\n",
       " 'quizás',\n",
       " 'quién',\n",
       " 'quiénes',\n",
       " 'qué',\n",
       " 'raras',\n",
       " 'realizado',\n",
       " 'realizar',\n",
       " 'realizó',\n",
       " 'repente',\n",
       " 'respecto',\n",
       " 'sabe',\n",
       " 'sabeis',\n",
       " 'sabemos',\n",
       " 'saben',\n",
       " 'saber',\n",
       " 'sabes',\n",
       " 'salvo',\n",
       " 'se',\n",
       " 'sea',\n",
       " 'sean',\n",
       " 'segun',\n",
       " 'segunda',\n",
       " 'segundo',\n",
       " 'según',\n",
       " 'seis',\n",
       " 'ser',\n",
       " 'sera',\n",
       " 'será',\n",
       " 'serán',\n",
       " 'sería',\n",
       " 'señaló',\n",
       " 'si',\n",
       " 'sido',\n",
       " 'siempre',\n",
       " 'siendo',\n",
       " 'siete',\n",
       " 'sigue',\n",
       " 'siguiente',\n",
       " 'sin',\n",
       " 'sino',\n",
       " 'sobre',\n",
       " 'sois',\n",
       " 'sola',\n",
       " 'solamente',\n",
       " 'solas',\n",
       " 'solo',\n",
       " 'solos',\n",
       " 'somos',\n",
       " 'son',\n",
       " 'soy',\n",
       " 'soyos',\n",
       " 'su',\n",
       " 'supuesto',\n",
       " 'sus',\n",
       " 'suya',\n",
       " 'suyas',\n",
       " 'suyo',\n",
       " 'sé',\n",
       " 'sí',\n",
       " 'sólo',\n",
       " 'tal',\n",
       " 'tambien',\n",
       " 'también',\n",
       " 'tampoco',\n",
       " 'tan',\n",
       " 'tanto',\n",
       " 'tarde',\n",
       " 'te',\n",
       " 'temprano',\n",
       " 'tendrá',\n",
       " 'tendrán',\n",
       " 'teneis',\n",
       " 'tenemos',\n",
       " 'tener',\n",
       " 'tenga',\n",
       " 'tengo',\n",
       " 'tenido',\n",
       " 'tenía',\n",
       " 'tercera',\n",
       " 'ti',\n",
       " 'tiempo',\n",
       " 'tiene',\n",
       " 'tienen',\n",
       " 'toda',\n",
       " 'todas',\n",
       " 'todavia',\n",
       " 'todavía',\n",
       " 'todo',\n",
       " 'todos',\n",
       " 'total',\n",
       " 'trabaja',\n",
       " 'trabajais',\n",
       " 'trabajamos',\n",
       " 'trabajan',\n",
       " 'trabajar',\n",
       " 'trabajas',\n",
       " 'trabajo',\n",
       " 'tras',\n",
       " 'trata',\n",
       " 'través',\n",
       " 'tres',\n",
       " 'tu',\n",
       " 'tus',\n",
       " 'tuvo',\n",
       " 'tuya',\n",
       " 'tuyas',\n",
       " 'tuyo',\n",
       " 'tuyos',\n",
       " 'tú',\n",
       " 'ultimo',\n",
       " 'un',\n",
       " 'una',\n",
       " 'unas',\n",
       " 'uno',\n",
       " 'unos',\n",
       " 'usa',\n",
       " 'usais',\n",
       " 'usamos',\n",
       " 'usan',\n",
       " 'usar',\n",
       " 'usas',\n",
       " 'uso',\n",
       " 'usted',\n",
       " 'ustedes',\n",
       " 'va',\n",
       " 'vais',\n",
       " 'valor',\n",
       " 'vamos',\n",
       " 'van',\n",
       " 'varias',\n",
       " 'varios',\n",
       " 'vaya',\n",
       " 'veces',\n",
       " 'ver',\n",
       " 'verdad',\n",
       " 'verdadera',\n",
       " 'verdadero',\n",
       " 'vez',\n",
       " 'vosotras',\n",
       " 'vosotros',\n",
       " 'voy',\n",
       " 'vuestra',\n",
       " 'vuestras',\n",
       " 'vuestro',\n",
       " 'vuestros',\n",
       " 'ya',\n",
       " 'yo',\n",
       " 'él',\n",
       " 'ésa',\n",
       " 'ésas',\n",
       " 'ése',\n",
       " 'ésos',\n",
       " 'ésta',\n",
       " 'éstas',\n",
       " 'éste',\n",
       " 'éstos',\n",
       " 'última',\n",
       " 'últimas',\n",
       " 'último',\n",
       " 'últimos'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y la lista de Stop Words\n",
    "nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Vemos que en el caso b no figuran las stop words. La diferencia parece estar por ese sector. Igualmente las palabras \"Garro\" y \"Bizco\" no se porque no figuran en ambas listas.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*En terminos de extra, seguimos al tutorial y recolectamos aquellas cosas que caracterizan a una palabra. En este caso: \"Manuel\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nuevo', 'duro', 'creyo', 'manana', 'altivar', 'noble', 'blanco', 'soso', 'nina', 'solo', 'propiciar', 'despues', 'comun', 'despues', 'enterar', 'albanil', 'alegrar', 'cuartar', 'repatriar', 'tias', 'escandaloso', 'solo', 'advertir', 'acompano', 'nuevo', 'insoportable', 'creyo', 'recorrio', 'irritar', 'indeciso']\n"
     ]
    }
   ],
   "source": [
    "print(obtener_adj_pers(doc.spacy_doc, \"Manuel\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:diplodatos-ayv]",
   "language": "python",
   "name": "conda-env-diplodatos-ayv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
