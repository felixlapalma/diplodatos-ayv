{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio: Analizar texto\n",
    "\n",
    "Utilizar un texto de proyecto Gutenberg en castellano (http://www.gutenberg.org/browse/languages/es)\n",
    "\n",
    "Contar palabras y ordenar por frecuencia\n",
    "- Limpiar preludio y licencia de Project Gutenberg\n",
    "- Omitir “palabras vacías” (stop words) y símbolos\n",
    "\n",
    "Encontrar personajes\n",
    "\n",
    "Hacer un análisis extra a gusto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# función auxiliar\n",
    "#https://relopezbriega.github.io/blog/2017/09/23/procesamiento-del-lenguaje-natural-con-python/\n",
    "def leer_texto(texto):\n",
    "    \"\"\"Funcion auxiliar para leer un archivo de texto\"\"\"\n",
    "    with open(texto, 'r') as text:\n",
    "        return text.read()\n",
    "    \n",
    "def inStr_idx(textin,strLst):\n",
    "    s_id_ini=[]\n",
    "    s_id_end=[]\n",
    "    for s in strLst:\n",
    "        s_len=len(s)\n",
    "        if str.find(textin,s)!=-1:\n",
    "            s_id_ini.append(str.find(textin,s))\n",
    "            s_id_end.append(str.find(textin,s)+s_len)\n",
    "        else:\n",
    "            s_id_ini.append(-1)\n",
    "            s_id_end.append(-1)\n",
    "    return s_id_ini,s_id_end\n",
    "\n",
    "#https://relopezbriega.github.io/blog/2017/09/23/procesamiento-del-lenguaje-natural-con-python/\n",
    "def encontrar_personajes(doc):\n",
    "    \"\"\"\n",
    "    Devuelve una lista de los personajes de un `doc` con su cantidad de\n",
    "    ocurrencias\n",
    "    \n",
    "    :param doc: NLP documento parseado por Spacy\n",
    "    :return: Lista de Tuplas con la forma\n",
    "        [('winston', 686), (\"o'brien\", 135), ('julia', 85),]\n",
    "    \"\"\"\n",
    "    personajes = Counter()\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PER':\n",
    "            personajes[ent.lemma_] += 1\n",
    "            \n",
    "    return personajes.most_common()\n",
    "\n",
    "#https://relopezbriega.github.io/blog/2017/09/23/procesamiento-del-lenguaje-natural-con-python/\n",
    "def obtener_adj_pers(doc, personaje):\n",
    "    \"\"\"\n",
    "    Encontrar todos los adjetivos relacionados a un personaje en un `doc`\n",
    "    \n",
    "    :param doc: NLP documento parseado por Spacy\n",
    "    :param personaje: un objeto String \n",
    "    :return: lista de adjetivos relacionados a un `personaje`\n",
    "    \"\"\"\n",
    "    \n",
    "    adjetivos = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.lemma_ == personaje:\n",
    "            for token in ent.subtree:\n",
    "                if token.pos_ == 'ADJ':\n",
    "                    adjetivos.append(token.lemma_)\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.lemma_ == personaje:\n",
    "            if ent.root.dep_ == 'nsubj':\n",
    "                for child in ent.root.head.children:\n",
    "                    if child.dep_ == 'acomp':\n",
    "                        adjetivos.append(child.lemma_)\n",
    "    \n",
    "    return adjetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Limpiar preludio y licencia de Project Gutenberg*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[762, 397086]\n",
      "[827, 397110]\n",
      "[-1, -1]\n",
      "[-1, -1]\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el texto\n",
    "texto_raw = leer_texto('pg43033.txt')\n",
    "# Vemos de limpiar la parte de proyecto Gutenberg. Si abrimos el documento observamos un par de frases en las cuales parecen marcar el limite\n",
    "# ['Nota del transcriptor: La ortografía del original fue conservada.','End of Project Gutenberg']\n",
    "# Entonces buscamos los indices final de la primera e inicial de la segunda y nos quedamos con la seccion del medio\n",
    "ini_idx,end_idx=inStr_idx(texto_raw,['Nota del transcriptor: La ortografía del original fue conservada.','End of Project Gutenberg'])\n",
    "print(ini_idx)\n",
    "print(end_idx)\n",
    "texto_ini=texto_raw[end_idx[0]:ini_idx[1]]\n",
    "# Y si los buscamos nuevamente no deberiamos encontrarlos (-1 en inStr_idx)\n",
    "ini_idx,end_idx=inStr_idx(texto_ini,['Nota del transcriptor: La ortografía del original fue conservada.','End of Project Gutenberg'])\n",
    "print(ini_idx)\n",
    "print(end_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la limpieza en el sentido automatico se podria buscar marcadores estandar para todos los libros y utlizarlos para remover las secciones correspondientes. En el presente caso \"End of Project Gutenberg\" parece ser un buen marcardor final, pero no me queda claro como remover la seccion inicial en forma automatica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Cargamos los modulos e instanciamos el modelo en español*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "# Cargando el modelo en español de spacy\n",
    "nlp = textacy.load_spacy('es_core_news_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Limpiamos un poco mas el texto. Seguimos *\n",
    "- https://textacy.readthedocs.io/en/stable/api_reference.html#module-textacy.preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizamos espacios en blanco\n",
    "texto_nwp=textacy.preprocess.normalize_whitespace(texto_ini)\n",
    "#Y Seguimos\n",
    "texto_prc=textacy.preprocess.preprocess_text(texto_nwp, fix_unicode=True, lowercase=False, transliterate=False, no_urls=False, no_emails=False, no_phone_numbers=True, no_numbers=False, no_currency_symbols=True, no_punct=True, no_contractions=False, no_accents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora lo incorporamos en un doc de textacy\n",
    "doc=textacy.Doc(texto_prc,lang=nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_terms=doc.to_bag_of_terms(ngrams=(1), named_entities=True, normalize=u'lemma', lemmatize=None, lowercase=None, weighting=u'freq', as_strings=True,filter_stops=True,filter_punct=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Las listas ordenadas por frecuencia serian:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De : https://www.pythoncentral.io/how-to-sort-python-dictionaries-by-key-or-value/\n",
    "bag_terms_list_sorted_min_max=sorted(bag_terms, key=bag_terms.__getitem__,reverse=False)\n",
    "bag_terms_list_sorted_max_min=sorted(bag_terms, key=bag_terms.__getitem__,reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Buscamos los personajes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PER', 'MISC', 'LOC', 'ORG']\n",
      "838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Bayardo',\n",
       " 'Comprendio',\n",
       " 'Maximo',\n",
       " 'Muley Edris',\n",
       " 'Pero Vidal',\n",
       " 'Quizas',\n",
       " 'Matansa',\n",
       " 'Manuel Asi asi Te',\n",
       " 'Don Alonso']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Usamos extract.named_entities\n",
    "ents=textacy.extract.named_entities(doc)\n",
    "ents_per=[]\n",
    "ents_lemma_set=[]\n",
    "for x in ents:\n",
    "    ents_lemma_set.append(x.label_)\n",
    "    #En https://textacy.readthedocs.io/en/stable/api_reference.html#module-textacy.extract menciona PERSON, pero al ver los posibles lemmas solo PER existe como opcion...\n",
    "    if x.label_=='PER':\n",
    "        ents_per.append(x.lemma_)\n",
    "ents_lemma_set=list(set(ents_lemma_set))\n",
    "ents_per=list(set(ents_per))\n",
    "print(ents_lemma_set)\n",
    "print(len(ents_per))\n",
    "ents_per[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Usamos alternativamente la funcion del tutorial. No me queda claro porque interpretaria ' ' como PER*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "844"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ents_per_func=encontrar_personajes(doc.spacy_doc)\n",
    "len(ents_per_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Vemos que tenemos alguna diferencia en el metodo. Veamos de donde sale...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "entlist=[x[0] for x in ents_per_func]\n",
    "diff_list_a=[];diff_list_b=[]\n",
    "for x in entlist:\n",
    "    if x in ents_per:\n",
    "        pass\n",
    "    else:\n",
    "        diff_list_a.append(x)\n",
    "for x in ents_per:\n",
    "    if x in entlist:\n",
    "        pass\n",
    "    else:\n",
    "        diff_list_b.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['El Bizco',\n",
       " 'El',\n",
       " 'La',\n",
       " 'El Garro',\n",
       " 'Un',\n",
       " 'Las \\n cuco',\n",
       " 'La baronesa de Aynant Paquita Figueroa',\n",
       " 'Los',\n",
       " 'Aquel Bayardo']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_list_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bayardo', 'baronesa de Aynant Paquita Figueroa', 'cuco']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_list_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['actualmente',\n",
       " 'paìs',\n",
       " 'dan',\n",
       " 'ciertas',\n",
       " 'decir',\n",
       " 'haya',\n",
       " 'eramos',\n",
       " 'estan',\n",
       " 'soy',\n",
       " 'realizó',\n",
       " 'al',\n",
       " 'vuestros',\n",
       " 'cuándo',\n",
       " 'somos',\n",
       " 'he',\n",
       " 'mismas',\n",
       " 'era',\n",
       " 'cuántas',\n",
       " 'misma',\n",
       " 'consideró']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y la lista de Stop Words\n",
    "stop_words=list(nlp.Defaults.stop_words)\n",
    "#Las primeras 20\n",
    "stop_words[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('El', 'OK'),\n",
       " ('Aquel', 'OK'),\n",
       " ('Las', 'OK'),\n",
       " ('La', 'OK'),\n",
       " ('Los', 'OK'),\n",
       " ('Un', 'OK')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Veamos que efectivamente \"El,La,Un,Los,La,Aquel\" son Stops Words.\n",
    "str_test=list(set([x.split()[0] for x in diff_list_a]))\n",
    "str_test\n",
    "str_in_stop_words=[]\n",
    "for x in str_test:\n",
    "    if x.lower() in stop_words:\n",
    "        str_in_stop_words.append((x,'OK'))\n",
    "    else:\n",
    "        str_in_stop_words.append((x,'FAIL'))\n",
    "str_in_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Vemos que en el caso b no figuran las stop words. La diferencia parece estar por ese sector. Igualmente las palabras \"Garro\" y \"Bizco\" no se porque no figuran en ambas listas.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*En terminos de extra, seguimos al tutorial y recolectamos aquellas cosas que caracterizan a una palabra. En este caso: \"Manuel\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nuevo', 'duro', 'creyo', 'manana', 'altivar', 'noble', 'blanco', 'soso', 'nina', 'solo', 'propiciar', 'despues', 'comun', 'despues', 'enterar', 'albanil', 'alegrar', 'cuartar', 'repatriar', 'tias', 'escandaloso', 'solo', 'advertir', 'acompano', 'nuevo', 'insoportable', 'creyo', 'recorrio', 'irritar', 'indeciso']\n"
     ]
    }
   ],
   "source": [
    "print(obtener_adj_pers(doc.spacy_doc, \"Manuel\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Otra cosa que parece resaltar es que se selecciona personajes por comenzar en mayuscula\n",
    "# Entonces si tomamos todo en minusculas no deberiamos ver escencialmente ningun PER\n",
    "texto_prc=textacy.preprocess.preprocess_text(texto_nwp, fix_unicode=True, lowercase=True, transliterate=False, no_urls=False, no_emails=False, no_phone_numbers=True, no_numbers=False, no_currency_symbols=True, no_punct=True, no_contractions=False, no_accents=True)\n",
    "# Ahora lo incorporamos en un doc de textacy\n",
    "doc=textacy.Doc(texto_prc,lang=nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PER', 'LOC', 'MISC', 'ORG']\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ents=textacy.extract.named_entities(doc)\n",
    "ents_per=[]\n",
    "ents_lemma_set=[]\n",
    "for x in ents:\n",
    "    ents_lemma_set.append(x.label_)\n",
    "    #En https://textacy.readthedocs.io/en/stable/api_reference.html#module-textacy.extract menciona PERSON, pero al ver los posibles lemmas solo PER existe como opcion...\n",
    "    if x.label_=='PER':\n",
    "        ents_per.append(x.lemma_)\n",
    "ents_lemma_set=list(set(ents_lemma_set))\n",
    "ents_per=list(set(ents_per))\n",
    "print(ents_lemma_set)\n",
    "print(len(ents_per))\n",
    "ents_per[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que es justamente lo que comentabamos en relacion a la seleccion de personajes por comenzar en mayuscula."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:diplodatos-ayv]",
   "language": "python",
   "name": "conda-env-diplodatos-ayv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
